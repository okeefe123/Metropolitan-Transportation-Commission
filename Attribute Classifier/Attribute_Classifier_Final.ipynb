{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "attached-connectivity",
   "metadata": {},
   "source": [
    "# Attribute Classifier\n",
    "\n",
    "### Goals:\n",
    "\n",
    "- Create a classification model that, when given a raw document, finds the probability that a given line holds relevance to a land use attribute.\n",
    "\n",
    "- Provide MTC with yet another metric-drive analytical tool to determine the standardization in the structure of policy documents from any jurisdiction.\n",
    "\n",
    "### Procedure:\n",
    "\n",
    "1. Preprocessing:\n",
    "    - Transform raw text version of policy document into a table\n",
    "    - Each row represents a line of the document.\n",
    "    - Initial features - city, line of the policy\n",
    "\n",
    "\n",
    "2. Feature Selection:\n",
    "    - Tokenize each row using spaCy and lemmatize all tokens\n",
    "    - Extract character count, character count, and average word length as numerical features\n",
    "    - Encode cities via mean frequency\n",
    "    - Vectorize the lemmatized tokens such that the model can interpret them during training/testing\n",
    "    - Encode labels in the training/test set\n",
    "    \n",
    "### Production Model:\n",
    "\n",
    "- Provides the probability of any given line in a document belonging to each of the assumed land use classes\n",
    "- Necessary Information Format:\n",
    "    1. Raw text version of policy zoning document\n",
    "    2. city_frequency.json - information of the count frequency of documents used during training stage\n",
    "    3. decode_labels.json - connection between integer classes and their corresponding land use attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imposed-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.pipeline                import Pipeline, FeatureUnion\n",
    "from sklearn.svm                     import LinearSVC      # baseline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from   sklearn.ensemble              import RandomForestClassifier\n",
    "from sklearn.preprocessing           import *\n",
    "from sklearn.impute                  import SimpleImputer\n",
    "from sklearn.compose                 import ColumnTransformer\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def spacy_tokenizer(string: str) -> str:\n",
    "    doc = nlp(string)\n",
    "    new_string = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return new_string\n",
    "\n",
    "def numerical_features(X_df: pd.DataFrame) -> None:\n",
    "    tokens = X_df['Context'].apply(lambda x: re.sub(r'[^\\w\\s]', '', spacy_tokenizer(x).strip()))\n",
    "    X_df['Char Count'] = tokens.apply(lambda x: len(x))\n",
    "    X_df['Word Count'] = tokens.apply(lambda x: len(x.split()))\n",
    "    X_df['Avg Word Length'] = X_df['Char Count'] / X_df['Word Count']\n",
    "    \n",
    "def encode_cities_mean_frequency(X_df: pd.DataFrame) -> None:\n",
    "    keys = X_df['City'].value_counts().index.values\n",
    "    vals = (X_df['City'].value_counts() / len(X_df)).values\n",
    "    encode_cities = dict(zip(keys, vals))\n",
    "    # For production use, will need to save the frequency of cities in the case of new cities being included\n",
    "    # when used.\n",
    "    city_frequency = dict(zip(keys, vals * len(X_df)))\n",
    "    \n",
    "\n",
    "    X_df['City'] = X_df['City'].map(lambda x: encode_cities[x])\n",
    "    \n",
    "    return city_frequency\n",
    "    \n",
    "def encode_label(y_df: pd.DataFrame) -> None:\n",
    "    encode_labels = {'max_dua'          : 0,\n",
    "                     'minimum_lot_sqft' : 1,\n",
    "                     'building_height'  : 2,\n",
    "                     'units_per_lot'    : 3,\n",
    "                     'max_far'          : 4, \n",
    "                     'none'             : 5\n",
    "    }\n",
    "\n",
    "    y_df['Attribute'] = y_df['Attribute'].map(lambda y: encode_labels[y])\n",
    "    \n",
    "def preprocess_pipeline(X_df, y_df):\n",
    "    print(\"Context Numerical Analysis\")\n",
    "    # Update X_df with number variable analysis\n",
    "    numerical_features(X_df)\n",
    "    \n",
    "    print(\"Hash Vectorizing...\")\n",
    "    # Tokenize the Context column into a sparse matrix\n",
    "    vectorizer = HashingVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "#     vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "    sparse = vectorizer.fit_transform(X_df['Context'])    \n",
    "    \n",
    "    print(\"Encoding cities...\")\n",
    "    #encode the \"cities\" feature\n",
    "    city_frequency = encode_cities_mean_frequency(X_df)\n",
    "    \n",
    "    print(\"Transforming sparse matrix...\")\n",
    "    # transform sparse CV matrix such that each dimension is given its own column\n",
    "    # drop context and join X_df with sparse (dataframe)\n",
    "    X_df = X_df.join(pd.DataFrame(sparse.todense())).drop(['Context'], axis=1)\n",
    "    \n",
    "    print(\"Encoding the labels...\")\n",
    "    #encode the labels\n",
    "    encode_label(y_df)\n",
    "    \n",
    "    return X_df, y_df, city_frequency\n",
    "\n",
    "def train_test_split(X_df, y_df, frac):\n",
    "    X = np.array(X_df)\n",
    "    y = np.array(y_df)\n",
    "    skf = StratifiedShuffleSplit(n_splits=2, test_size=(1-frac))\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "canadian-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_from_pipeline = pd.read_csv('ML-Modeling-Data/X_from_pipeline.csv', index_col='Unnamed: 0')\n",
    "y_from_pipeline = pd.read_csv('ML-Modeling-Data/y_from_pipeline.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "republican-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_from_pipeline.shape[0] == y_from_pipeline.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-annex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Numerical Analysis\n",
      "Hash Vectorizing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okeefe/.pyenv/versions/3.8.1/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding cities...\n",
      "Transforming sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "X_df, y_df, city_frequency = preprocess_pipeline(X_from_pipeline, y_from_pipeline)\n",
    "\n",
    "with open('ML-Modeling-Data/city_frequency.json', 'w') as outfile:\n",
    "    json.dump(city_frequency, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_df, y_df, frac=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "current-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape[0] + X_val.shape[0] == X_df.shape[0]\n",
    "assert y_train.shape[0] + y_val.shape[0] == y_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "welcome-customer",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okeefe/.pyenv/versions/3.8.1/lib/python3.8/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ccp_alpha': 0.0,\n",
       " 'criterion': 'friedman_mse',\n",
       " 'init': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 5,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 500,\n",
       " 'n_iter_no_change': None,\n",
       " 'random_state': None,\n",
       " 'subsample': 0.5,\n",
       " 'tol': 0.0001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble                import GradientBoostingClassifier\n",
    "from sklearn.model_selection         import RandomizedSearchCV\n",
    "from sklearn.svm                     import LinearSVC\n",
    "from sklearn                         import metrics\n",
    "import matplotlib.pyplot             as plt\n",
    "\n",
    "\n",
    "search_space = {'learning_rate'    : [0.1, 0.001, 0.0001, 0.00001],\n",
    "                'max_depth'        : [2, 3, 4, 5],\n",
    "                'min_samples_leaf' : [1, 2, 4, 6],\n",
    "                'n_estimators'     : [10, 20, 50, 100, 150, 200, 500],\n",
    "                'subsample'        : [0.2, 0.4, 0.5, 0.6, 0.8, 0.9]\n",
    "                }\n",
    "\n",
    "clf_random = RandomizedSearchCV(estimator=GradientBoostingClassifier(),\n",
    "                                param_distributions=search_space,\n",
    "                                n_iter=50,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1,\n",
    "                                verbose=1)\n",
    "\n",
    "best_model = clf_random.fit(X_train, y_train)\n",
    "best_model.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "better-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "         max_dua       0.88      0.80      0.83      1801\n",
      "minimum_lot_sqft       0.83      0.75      0.79      1295\n",
      " building_height       0.74      0.79      0.76       622\n",
      "   units_per_lot       0.63      0.76      0.69       366\n",
      "         max_far       0.65      0.80      0.72       481\n",
      "            none       0.75      0.82      0.79       535\n",
      "\n",
      "        accuracy                           0.78      5100\n",
      "       macro avg       0.74      0.79      0.76      5100\n",
      "    weighted avg       0.79      0.78      0.79      5100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_model.predict(X_val)\n",
    "\n",
    "encode_labels = {'max_dua'          : 0,\n",
    "                 'minimum_lot_sqft' : 1,\n",
    "                 'building_height'  : 2,\n",
    "                 'units_per_lot'    : 3,\n",
    "                 'max_far'          : 4, \n",
    "                 'none'             : 5\n",
    "}\n",
    "\n",
    "print(metrics.classification_report(y_pred, y_val, target_names=encode_labels.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "complimentary-opportunity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report using CountVectorizer:\n",
    "\n",
    "#                   precision    recall  f1-score   support\n",
    "\n",
    "#          max_dua       0.88      0.80      0.83      1801\n",
    "# minimum_lot_sqft       0.83      0.75      0.79      1295\n",
    "#  building_height       0.74      0.79      0.76       622\n",
    "#    units_per_lot       0.63      0.76      0.69       366\n",
    "#          max_far       0.65      0.80      0.72       481\n",
    "#             none       0.75      0.82      0.79       535\n",
    "\n",
    "#         accuracy                           0.78      5100\n",
    "#        macro avg       0.74      0.79      0.76      5100\n",
    "#     weighted avg       0.79      0.78      0.79      5100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "compact-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'attrib_classifier_model_cvalidated.sav'\n",
    "pickle.dump(best_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-bracket",
   "metadata": {},
   "source": [
    "# Production Model Procedure/ Use Case:\n",
    "\n",
    "- Our use case will be uploading the policy information for Calistoga. See the \"Attribute_Classifier_RD\" notebook for more information on rationale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-fountain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
